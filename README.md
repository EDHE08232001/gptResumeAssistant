# GPT-Resume-Assistant

A FastAPI-based web service that generates concise, AI-powered summaries of a user’s résumé using OpenAI’s latest **`gpt-5-nano`** model.

---

## 🚀 Overview

This project exposes a simple REST API endpoint:

```

POST /api/chat

```

that accepts a chat‐style message list and responds with a résumé summary generated by **gpt-5-nano** through the **OpenAI Responses API**.

The backend is written in **Python 3.10** using:

* **FastAPI** – high-performance ASGI framework
* **Gunicorn** – process manager for production
* **OpenAI Python SDK** – to call the new `responses` API
* **Azure App Service** – hosting and continuous deployment

---

## 🏗 Project Structure

```

.
├─ main.py              # FastAPI entrypoint (ASGI app)
├─ src/
│   └─ gptAssistant.py  # Service layer to call OpenAI
├─ public/              # Static files (if any)
├─ requirements.txt
└─ .env                 # Local environment variables

```

---

## ⚠️ Challenges Encountered

During deployment to **Azure App Service (Linux)** several issues surfaced:

### 1. Missing ASGI Worker Dependency
* **Symptom:** Azure logs showed  
```

ModuleNotFoundError: No module named 'uvicorn'

```
* **Cause:** FastAPI is an **ASGI** framework. Azure’s runtime auto-detects ASGI apps and tries to run  
`gunicorn -k uvicorn.workers.UvicornWorker …`.  
Even though the code never explicitly imported `uvicorn`, the worker class requires it.

### 2. Startup Command Ambiguity
* Initially used:
```

gunicorn main\:app --bind=0.0.0.0:\$PORT

````
* While valid for WSGI apps (e.g. Flask), this left Azure free to override it with its own ASGI detection, again expecting the `uvicorn` worker.

### 3. OpenAI SDK Misunderstandings
* Early confusion about which API endpoint to use.  
* `gpt-5-nano` is supported **only** through the newer `client.responses.create()` method, not the older `chat.completions` API.

---

## 🛠 Solutions Implemented

1. **Added `uvicorn` to requirements**  
 ```bash
 uvicorn[standard]
````

Ensures the required worker is available regardless of Azure’s auto-detection.

2. **Pinned the Startup Command**
   In Azure Portal → *Configuration → General settings → Startup Command*:

   ```bash
   gunicorn main:app -w 1 -k uvicorn.workers.UvicornWorker --bind=0.0.0.0:$PORT
   ```

   This explicitly runs Gunicorn with the Uvicorn ASGI worker, the standard production pattern for FastAPI.

3. **Confirmed correct OpenAI usage**

   ```python
   from openai import OpenAI
   client = OpenAI()
   completion = client.responses.create(
       model="gpt-5-nano",
       input=[{"role": "user", "content": "Summarize my resume."}]
   )
   print(completion.output_text)
   ```

   The `responses` API works properly with the `gpt-5-nano` model.

4. **Redeployment**

   ```bash
   zip -r app.zip . -x "venv/*" ".git/*" "*.DS_Store" "app.zip" && \
   az webapp deploy --resource-group gptResumeRG \
       --name gpt-resume-assistant --src-path app.zip --type zip
   ```

---

## ✅ Key Takeaways

* **FastAPI ≠ Flask** – it needs an ASGI server.
* **Azure App Service** may auto-detect ASGI and silently require `uvicorn`; always include it or pin the startup command yourself.
* **OpenAI’s `gpt-5-nano`** works through the **Responses API**, not the legacy `chat.completions` endpoint.

---

## 🔗 Live App

Deployed at: [https://gpt-resume-assistant.azurewebsites.net](https://gpt-resume-assistant.azurewebsites.net)

Send a POST request to `/api/chat` with a JSON body of messages to receive a résumé summary powered by **gpt-5-nano**.

---

*Built and debugged with patience, perseverance, and just a hint of royal flair.* 👑
