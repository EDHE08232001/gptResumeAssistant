# GPT-Resume-Assistant

A FastAPI-based web service that generates concise, AI-powered summaries of a userâ€™s rÃ©sumÃ© using OpenAIâ€™s latest **`gpt-5-nano`** model.

---

## ğŸš€ Overview

This project exposes a simple REST API endpoint:

```

POST /api/chat

```

that accepts a chatâ€style message list and responds with a rÃ©sumÃ© summary generated by **gpt-5-nano** through the **OpenAI Responses API**.

The backend is written in **Python 3.10** using:

* **FastAPI** â€“ high-performance ASGI framework
* **Gunicorn** â€“ process manager for production
* **OpenAI Python SDK** â€“ to call the new `responses` API
* **Azure App Service** â€“ hosting and continuous deployment

---

## ğŸ— Project Structure

```

.
â”œâ”€ main.py              # FastAPI entrypoint (ASGI app)
â”œâ”€ src/
â”‚   â””â”€ gptAssistant.py  # Service layer to call OpenAI
â”œâ”€ public/              # Static files (if any)
â”œâ”€ requirements.txt
â””â”€ .env                 # Local environment variables

```

---

## âš ï¸ Challenges Encountered

During deployment to **Azure App Service (Linux)** several issues surfaced:

### 1. Missing ASGI Worker Dependency
* **Symptom:** Azure logs showed  
```

ModuleNotFoundError: No module named 'uvicorn'

```
* **Cause:** FastAPI is an **ASGI** framework. Azureâ€™s runtime auto-detects ASGI apps and tries to run  
`gunicorn -k uvicorn.workers.UvicornWorker â€¦`.  
Even though the code never explicitly imported `uvicorn`, the worker class requires it.

### 2. Startup Command Ambiguity
* Initially used:
```

gunicorn main\:app --bind=0.0.0.0:\$PORT

````
* While valid for WSGI apps (e.g. Flask), this left Azure free to override it with its own ASGI detection, again expecting the `uvicorn` worker.

### 3. OpenAI SDK Misunderstandings
* Early confusion about which API endpoint to use.  
* `gpt-5-nano` is supported **only** through the newer `client.responses.create()` method, not the older `chat.completions` API.

---

## ğŸ›  Solutions Implemented

1. **Added `uvicorn` to requirements**  
 ```bash
 uvicorn[standard]
````

Ensures the required worker is available regardless of Azureâ€™s auto-detection.

2. **Pinned the Startup Command**
   In Azure Portal â†’ *Configuration â†’ General settings â†’ Startup Command*:

   ```bash
   gunicorn main:app -w 1 -k uvicorn.workers.UvicornWorker --bind=0.0.0.0:$PORT
   ```

   This explicitly runs Gunicorn with the Uvicorn ASGI worker, the standard production pattern for FastAPI.

3. **Confirmed correct OpenAI usage**

   ```python
   from openai import OpenAI
   client = OpenAI()
   completion = client.responses.create(
       model="gpt-5-nano",
       input=[{"role": "user", "content": "Summarize my resume."}]
   )
   print(completion.output_text)
   ```

   The `responses` API works properly with the `gpt-5-nano` model.

4. **Redeployment**

   ```bash
   zip -r app.zip . -x "venv/*" ".git/*" "*.DS_Store" "app.zip" && \
   az webapp deploy --resource-group gptResumeRG \
       --name gpt-resume-assistant --src-path app.zip --type zip
   ```

---

## âœ… Key Takeaways

* **FastAPI â‰  Flask** â€“ it needs an ASGI server.
* **Azure App Service** may auto-detect ASGI and silently require `uvicorn`; always include it or pin the startup command yourself.
* **OpenAIâ€™s `gpt-5-nano`** works through the **Responses API**, not the legacy `chat.completions` endpoint.

---

## ğŸ”— Live App

Deployed at: [https://gpt-resume-assistant.azurewebsites.net](https://gpt-resume-assistant.azurewebsites.net)

Send a POST request to `/api/chat` with a JSON body of messages to receive a rÃ©sumÃ© summary powered by **gpt-5-nano**.

---

*Built and debugged with patience, perseverance, and just a hint of royal flair.* ğŸ‘‘
